{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models were run on python without Jupyter notebooks on the HPC in a job queue, we have the results from the model in a csv and the logs from training in .err and .out files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "import transformers\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import logging\n",
    "import csv\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the TOKENIZERS_PARALLELISM environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    logger.info(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    logger.info(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    logger.info(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, cache_dir):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, cache_dir=cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True, cache_dir=cache_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # Set padding side to right\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data into a form that the model can consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_dataset(dataset_name, cache_dir):\n",
    "    dataset = load_dataset(dataset_name, \"en\", cache_dir=cache_dir, trust_remote_code=True)\n",
    "    def remove_extra_columns(example):\n",
    "        example[\"entities\"] = [entity[\"surfaceform\"] for entity in example[\"entities\"]]\n",
    "        example[\"relations\"] = [\n",
    "            {\n",
    "                \"subject\": example[\"entities\"][relation[\"subject\"]],\n",
    "                \"predicate\": relation[\"predicate\"],\n",
    "                \"object\": example[\"entities\"][relation[\"object\"]],\n",
    "            }\n",
    "            for relation in example[\"relations\"]\n",
    "        ]\n",
    "        return example\n",
    "    return dataset.map(remove_extra_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the input to our models into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data_point):\n",
    "    \"\"\"\n",
    "    Convert entities and relations to the expected output text format.\n",
    "    \"\"\"\n",
    "    query = data_point['text']\n",
    "    entities = \", \".join([f'\"{entity}\"' for entity in data_point['entities']])\n",
    "    relations = \"\\n\".join([f'\"{relations}\"' for relations in data_point['relations']])\n",
    "\n",
    "    text = f\"Given the following text, identify and extract all entities and their relations. Query: {query}\\n Entities: [{entities}]\\nRelations:\\n{relations}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding all the layers that LORA can be applied to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Gemma 7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cuda()\n",
    "\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "cache_dir = \"SLM/gemma2b\"  # Specify an alternative cache directory\n",
    "dataset_name = \"Babelscape/SREDFM\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, cache_dir)\n",
    "dataset = load_and_clean_dataset(dataset_name, cache_dir)\n",
    "\n",
    "tokenized_datasets = dataset.map(lambda dp: {\"model_input\": preprocess_function(dp)})\n",
    "logger.info(tokenized_datasets[\"test\"][0])\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "logger.info(modules)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "logger.info(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    dataset_text_field='model_input',\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,  # Increase batch size if memory allows\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=100,  # Add warmup steps\n",
    "        max_steps=1000,  # Increase max steps for better training\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,  # Log less frequently to reduce overhead\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,  # Use mixed precision training\n",
    "        dataloader_num_workers=4,  # Optimize data loading\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "new_model = \"gemma2b-trained\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    cache_dir=\"SLM/gemma2b_trained\"  # Specify an alternative cache directory\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.to(\"cpu\")\n",
    "tokenizer.save_pretrained(\"gemma2b_trained\")\n",
    "merged_model.save_pretrained(\"gemma2b_trained\", safe_serialization=True)\n",
    "\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Gemma7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cuda()\n",
    "\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "cache_dir = \"SLM/gemma7b\"  # Specify an alternative cache directory\n",
    "dataset_name = \"Babelscape/SREDFM\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, cache_dir)\n",
    "dataset = load_and_clean_dataset(dataset_name, cache_dir)\n",
    "\n",
    "tokenized_datasets = dataset.map(lambda dp: {\"model_input\": preprocess_function(dp)})\n",
    "logger.info(tokenized_datasets[\"test\"][0])\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "logger.info(modules)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "logger.info(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    dataset_text_field='model_input',\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,  # Increase batch size if memory allows\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=100,  # Add warmup steps\n",
    "        max_steps=1000,  # Increase max steps for better training\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,  # Log less frequently to reduce overhead\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,  # Use mixed precision training\n",
    "        dataloader_num_workers=4,  # Optimize data loading\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "new_model = \"gemma7b-trained\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    cache_dir=\"SLM/gemma7b_trained\"  # Specify an alternative cache directory\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.to(\"cpu\")\n",
    "tokenizer.save_pretrained(\"gemma7b_trained\")\n",
    "merged_model.save_pretrained(\"gemma7b_trained\", safe_serialization=True)\n",
    "\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling model results into a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models have been pushed to hugging-face and can be pulled from there for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a dataloader for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_batch(queries, model, tokenizer, device):\n",
    "    prompt_template = \"\"\"\n",
    "    <start_of_turn>user\n",
    "    Given the following text, identify and extract all entities and their relations.\n",
    "    {query}\n",
    "    <end_of_turn>\\n<start_of_turn>model\n",
    "    \"\"\"\n",
    "    # Generate prompts\n",
    "    prompts = [prompt_template.format(query=query) for query in queries]\n",
    "    encodeds = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",  # Explicitly specify padding\n",
    "        truncation=True,\n",
    "        max_length=1024  # Adjust based on your model's max input length\n",
    "    )\n",
    "    encodeds = {key: value.to(device) for key, value in encodeds.items()}\n",
    "\n",
    "    # Mixed precision inference for speedup\n",
    "    generated_ids = model.generate(\n",
    "            **encodeds,\n",
    "            max_new_tokens=500,  # Adjust as per expected output length\n",
    "            do_sample=False,     # Disable sampling for faster, deterministic results\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    outputs = []\n",
    "    for ids in generated_ids:\n",
    "        full_output = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        if \"Entities and Relations:\" in full_output:\n",
    "            relevant_part = full_output.split(\"Entities and Relations:\")[1].strip()\n",
    "            outputs.append(relevant_part)\n",
    "        else:\n",
    "            outputs.append(full_output.strip())  # Fallback to entire output if section is missing\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading first 1000 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Babelscape/SREDFM\", \"en\", cache_dir=\"SLM/datasets\", trust_remote_code=True)\n",
    "test_dataset = dataset['test'].select(range(1000))  # Select the first 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, true_entities, true_relations):\n",
    "        self.texts = texts\n",
    "        self.true_entities = true_entities\n",
    "        self.true_relations = true_relations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"true_entities\": self.true_entities[idx],\n",
    "            \"true_relations\": self.true_relations[idx],\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    true_entities = [item[\"true_entities\"] for item in batch]\n",
    "    true_relations = [item[\"true_relations\"] for item in batch]\n",
    "    return texts, true_entities, true_relations\n",
    "\n",
    "query_dataset = QueryDataset(\n",
    "    texts=test_dataset['text'],\n",
    "    true_entities=test_dataset['entities'],\n",
    "    true_relations=test_dataset['relations']\n",
    ")\n",
    "batch_size = 16  # Adjust based on available GPU memory\n",
    "dataloader = DataLoader(query_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 2b inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_path = \"Chinmay0701/gemma2b-trained\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=\"gemma2b-trained\", padding_size = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "tokenizer.padding_side = \"left\"  # Fix padding side for decoder-only models\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32, \n",
    "    cache_dir=\"gemma2b-trained\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "output_file = \"gemma2b_entities_relations.csv\"\n",
    "\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Query\", \"Predicted Entities\", \"Predicted Relations\", \"Output\"])  # Write header row\n",
    "\n",
    "    for batch_idx, (queries, true_entities, true_relations) in enumerate(dataloader):\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(dataloader)}...\")\n",
    "        \n",
    "        # Get predictions for the batch\n",
    "        predicted = get_completion_batch(queries, model, tokenizer, device)\n",
    "\n",
    "        # Split predicted results into entities and relations\n",
    "        for query, pred_output, true_ent, true_rel in zip(queries, predicted, true_entities, true_relations):\n",
    "            if \"Entities:\" in pred_output and \"Relations:\" in pred_output:\n",
    "            # Split the predicted output into entities and relations parts\n",
    "                parts = pred_output.split(\"Relations:\")\n",
    "                pred_entities = parts[0].split(\"Entities:\")\n",
    "                pred_entities = pred_entities[1].replace(\"Entities:\", \"\").strip()\n",
    "                pred_relations = parts[1].strip()\n",
    "            else:\n",
    "                pred_entities = \"NA\"\n",
    "                pred_relations = \"NA\"\n",
    "            \n",
    "            writer.writerow([query,pred_output ,pred_entities, pred_relations])\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 7b Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Chinmay0701/gemma7b-trained\"  # Path where the model and tokenizer are saved\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=\"gemma7b-trained\", padding_size = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "tokenizer.padding_side = \"left\" \n",
    "\n",
    "output_file = \"gemma7b_entities_relations.csv\"\n",
    "\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Query\", \"Predicted Entities\", \"Predicted Relations\", \"Output\"])  # Write header row\n",
    "\n",
    "    for batch_idx, (queries, true_entities, true_relations) in enumerate(dataloader):\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(dataloader)}...\")\n",
    "        \n",
    "        # Get predictions for the batch\n",
    "        predicted = get_completion_batch(queries, model, tokenizer, device)\n",
    "\n",
    "        # Split predicted results into entities and relations\n",
    "        for query, pred_output, true_ent, true_rel in zip(queries, predicted, true_entities, true_relations):\n",
    "            if \"Entities:\" in pred_output and \"Relations:\" in pred_output:\n",
    "            # Split the predicted output into entities and relations parts\n",
    "                parts = pred_output.split(\"Relations:\")\n",
    "                pred_entities = parts[0].split(\"Entities:\")\n",
    "                pred_entities = pred_entities[1].replace(\"Entities:\", \"\").strip()\n",
    "                pred_relations = parts[1].strip()\n",
    "            else:\n",
    "                pred_entities = \"NA\"\n",
    "                pred_relations = \"NA\"\n",
    "            \n",
    "            writer.writerow([query,pred_output ,pred_entities, pred_relations])\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebel inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(text: str) -> list:\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'subject': subject.strip(), 'predicate': relation.strip(),'object': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'subject': subject.strip(), 'predicate': relation.strip(),'object': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'subject': subject.strip(), 'predicate': relation.strip(),'object': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 256,\n",
    "    \"length_penalty\": 0,\n",
    "    \"num_beams\": 3,\n",
    "    \"num_return_sequences\": 3,\n",
    "}\n",
    "\n",
    "limit = 1000\n",
    "test_dataset = load_dataset(\"Babelscape/SREDFM\", \"en\", split=\"test\", trust_remote_code=True)\n",
    "test_dataset = test_dataset.map(remove_extra_columns)\n",
    "entities = test_dataset[\"entities\"][:limit]\n",
    "relations = test_dataset[\"relations\"][:limit]\n",
    "texts = test_dataset[\"text\"][:limit]\n",
    "\n",
    "node_scores = []\n",
    "relation_scores = []\n",
    "output_file = \"rebel_entities_relations.csv\"\n",
    "\n",
    "# Clear the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "for text, true_entity, true_relation in zip(texts, entities, relations):\n",
    "    model_inputs = tokenizer(text, max_length=256, padding=True, truncation=True, \n",
    "                             return_tensors = 'pt')\n",
    "    generated_tokens = model.generate(model_inputs[\"input_ids\"].to(model.device),\n",
    "                                    attention_mask=model_inputs[\"attention_mask\"].\n",
    "                                    to(model.device), **gen_kwargs)\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    predicted_entities = set()\n",
    "    predicted_relations = []\n",
    "    for idx, sentence in enumerate(decoded_preds):\n",
    "        new_relations = extract_triplets(sentence)\n",
    "        predicted_relations.extend(new_relations)\n",
    "        for relation in new_relations:\n",
    "            predicted_entities.add(relation['subject'])\n",
    "            predicted_entities.add(relation['object'])\n",
    "\n",
    "    with open(, \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(list(predicted_entities)) + \"\\n\")\n",
    "        f.write(str(predicted_relations) + \"\\n\")\n",
    "        f.write(\"\\n#\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse output before evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "    (\"'Non\", \"Non\"),\n",
    "    (\"[\\\"\\\"The First Celestial\", \"\\\"\\\"The First Celestial\"),\n",
    "    (\"[\\\"\\\"The Silent Serin\", \"\\\"\\\"The Silent Serin\"),\n",
    "    (\"[\\\"\\\"The Untuned Ventriloquist\", \"\\\"\\\"The Untuned Ventriloquist\"),\n",
    "    (\"[\\\"\\\"The Cannibal Manifesto in the Dark\", \"\\\"\\\"The Cannibal Manifesto in the Dark\")\n",
    "]\n",
    "\n",
    "def parse_gemma_output(file_path: str) -> dict:\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Clean ' from the data\n",
    "    data = re.sub(r\"([a-zA-Z0-9])'([a-zA-Z0-9\\s-])\", r\"\\1\\2\", data)\n",
    "    for replacement in replacements:\n",
    "        data = data.replace(replacement[0], replacement[1])\n",
    "    sections = data.split('[')\n",
    "    all_entities = []\n",
    "    all_relations = []\n",
    "    skip = 0\n",
    "    first_section = True\n",
    "    for s in sections[1:]:\n",
    "        if s[:1] == \"{\" or s[:1].isalnum() or s[:1] in \".=\":\n",
    "            if s[:1] == \"{\":\n",
    "                skip += 1\n",
    "            if not s[:1].isalnum() and not s[:1] in \".=\" and s[:1] != \"{\":\n",
    "                print(s)\n",
    "            continue\n",
    "\n",
    "        # Handle cases where model didnt generate anything\n",
    "        if skip % 2 != 0:\n",
    "            raise ValueError(\"Unbalanced brackets\")\n",
    "        count = skip // 2 - 1\n",
    "        if first_section:\n",
    "            count += 1\n",
    "            first_section = False\n",
    "        for _ in range(count):\n",
    "            all_entities.append([])\n",
    "            all_relations.append([])\n",
    "        skip = 0\n",
    "\n",
    "        end_index = s.find(']')\n",
    "        if end_index != -1:\n",
    "            entities = s[:end_index].replace(\"\\\"\\\"\", \"\").split(', ')\n",
    "            relations_as_list = s[end_index + 4:].replace(\"\\\"\\\"\", \"\").split('\\n')\n",
    "            if \"}\" not in relations_as_list[-1]:\n",
    "                relations_as_list = relations_as_list[:-1]\n",
    "            else:\n",
    "                relations_as_list[-1] = relations_as_list[-1][:relations_as_list[-1].find(\"}\") + 1]\n",
    "            relations = list(set(relations_as_list))\n",
    "            try:\n",
    "                relations = [eval(relation) for relation in relations if relation]\n",
    "            except SyntaxError as _:\n",
    "                # print(f\"Error parsing the following relations {relations}\")\n",
    "                all_entities.append([])\n",
    "                all_relations.append([])\n",
    "                continue\n",
    "\n",
    "            all_entities.append(entities)\n",
    "            all_relations.append(relations)\n",
    "    return {\"entities\": all_entities, \"relations\": all_relations}\n",
    "\n",
    "def parse_rebel_output(file_path: str) -> dict:\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    sections = data.split('\\n#\\n')\n",
    "    all_entities = []\n",
    "    all_relations = []\n",
    "    for s in sections[:-1]:\n",
    "        s = s.strip()\n",
    "        entities, relations = s.split('\\n')\n",
    "        all_entities.append(eval(entities))\n",
    "        all_relations.append(eval(relations))\n",
    "\n",
    "    return {\"entities\": all_entities, \"relations\": all_relations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_metrics(pred_nodes, true_nodes, pred_relations, true_relations):\n",
    "    # Compute for nodes\n",
    "    if len(pred_nodes) == 0:\n",
    "        node_f1 = 0\n",
    "    else:\n",
    "        node_precision = len(set(pred_nodes) & set(true_nodes)) / len(set(pred_nodes))\n",
    "        node_recall = len(set(pred_nodes) & set(true_nodes)) / len(set(true_nodes))\n",
    "        node_f1 = 2 * node_precision * node_recall / (node_precision + node_recall) if\\\n",
    "                (node_precision + node_recall) > 0 else 0\n",
    "\n",
    "    # Compute for relationships\n",
    "    if len(pred_relations) == 0:\n",
    "        relation_f1 = 0\n",
    "    else:\n",
    "        relation_precision = len(set(pred_relations) & set(true_relations)) / len(set(pred_relations))\n",
    "        relation_recall = len(set(pred_relations) & set(true_relations)) / len(set(true_relations))\n",
    "        relation_f1 = 2 * relation_precision * relation_recall / (relation_precision + relation_recall) if\\\n",
    "                (relation_precision + relation_recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"node_f1\": node_f1,\n",
    "        \"relation_f1\": relation_f1,\n",
    "    }\n",
    "\n",
    "def evaluate(true_nodes: list, predicted_nodes: list, true_relations: dict, predicted_relations: dict) -> dict:\n",
    "    \"\"\"Takes in the true and predicted nodes and relations and returns the node and relation scores\n",
    "    true_nodes: list of true nodes represented as strings\n",
    "    predicted_nodes: list of predicted nodes represented as strings\n",
    "    true_relations: dictionary of true relations in format {'subject': str, 'predicate': str, 'object': str}\n",
    "    predicted_relations: dictionary of predicted relations in format {'subject': str, 'predicate': str, 'object': str}\n",
    "    \"\"\"\n",
    "\n",
    "    if not predicted_nodes and not predicted_relations:\n",
    "        return {'node_score': 0, 'relations_score': 0}\n",
    "\n",
    "    true_relations_tuples = [(rel['subject'], rel['predicate'], rel['object'])\n",
    "                             for rel in true_relations]\n",
    "    predicted_relations_tuples = [(rel['subject'], rel['predicate'], rel['object'])\n",
    "                                  for rel in predicted_relations]\n",
    "\n",
    "    true_relations_text = [f\"{rel['subject']} -> {rel['predicate']} -> {rel['object']}\"\n",
    "                            for rel in true_relations]\n",
    "    predicted_relations_text = [f\"{rel['subject']} -> {rel['predicate']} -> {rel['object']}\"\n",
    "                                for rel in predicted_relations]\n",
    "\n",
    "    metrics = compute_graph_metrics(predicted_nodes, true_nodes,\n",
    "                                    predicted_relations_tuples, true_relations_tuples)\n",
    "\n",
    "    # Load embedding model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    true_embeddings = model.encode(true_relations_text)\n",
    "    pred_embeddings = model.encode(predicted_relations_text)\n",
    "\n",
    "    if len(pred_embeddings) > 0:\n",
    "        aligned_similarity = cosine_similarity(true_embeddings, pred_embeddings)\n",
    "        aligned_similarity = [max(row) for row in aligned_similarity]\n",
    "\n",
    "        aligned_similarity_mean = sum(aligned_similarity) / len(aligned_similarity)\n",
    "    else:\n",
    "        aligned_similarity_mean = 0\n",
    "\n",
    "    metrics[\"aligned_similarity\"] = aligned_similarity_mean\n",
    "\n",
    "    f1_weight = 0.3\n",
    "    relations_score = metrics[\"relation_f1\"] * f1_weight +\\\n",
    "        metrics[\"aligned_similarity\"] * (1-f1_weight)\n",
    "    return {'node_score': metrics['node_f1'], 'relations_score': relations_score}\n",
    "\n",
    "to_parse = [\n",
    "    (\"gemma2b_entities_relations.csv\", parse_gemma_output, \"Gemma 2b\"),\n",
    "    (\"gemma7b_entities_relations.csv\", parse_gemma_output, \"Gemma 7b\"),\n",
    "    (\"rebel_entities_relations.csv\", parse_rebel_output, \"Rebel\"),\n",
    "]\n",
    "\n",
    "for file_path, parser, name in to_parse:\n",
    "    node_scores = []\n",
    "    relations_scores = []\n",
    "    results = parser(file_path)\n",
    "    for true_entities, true_relations, pred_entities, pred_relations in zip(\n",
    "        entities, relations, results['entities'][start:end], results['relations'][start:end]\n",
    "        ):\n",
    "        scores = evaluate(true_entities, pred_entities, true_relations, pred_relations)\n",
    "        node_scores.append(scores['node_score'])\n",
    "        relations_scores.append(scores['relations_score'])\n",
    "    print(f\"{name} node score: {sum(node_scores) / len(node_scores)}\")\n",
    "    print(f\"{name} relations score: {sum(relations_scores) / len(relations_scores)}\")\n",
    "    print(f\"{name} node score without missing values: {sum(node_scores) / len([score for score in node_scores if score > 0])}\")\n",
    "    print(f\"{name} relations score without missing values: {sum(relations_scores) / len([score for score in relations_scores if score > 0])}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
