{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models were run on python without Jupyter notebooks on the HPC in a job queue, we have the results from the model in a csv and the logs from training in .err and .out files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "import transformers\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import logging\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the TOKENIZERS_PARALLELISM environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cuda():\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    logger.info(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    logger.info(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    logger.info(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, cache_dir):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, cache_dir=cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True, cache_dir=cache_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # Set padding side to right\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data into a form that the model can consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_dataset(dataset_name, cache_dir):\n",
    "    dataset = load_dataset(dataset_name, \"en\", cache_dir=cache_dir, trust_remote_code=True)\n",
    "    def remove_extra_columns(example):\n",
    "        example[\"entities\"] = [entity[\"surfaceform\"] for entity in example[\"entities\"]]\n",
    "        example[\"relations\"] = [\n",
    "            {\n",
    "                \"subject\": example[\"entities\"][relation[\"subject\"]],\n",
    "                \"predicate\": relation[\"predicate\"],\n",
    "                \"object\": example[\"entities\"][relation[\"object\"]],\n",
    "            }\n",
    "            for relation in example[\"relations\"]\n",
    "        ]\n",
    "        return example\n",
    "    return dataset.map(remove_extra_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the input to our models into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data_point):\n",
    "    \"\"\"\n",
    "    Convert entities and relations to the expected output text format.\n",
    "    \"\"\"\n",
    "    query = data_point['text']\n",
    "    entities = \", \".join([f'\"{entity}\"' for entity in data_point['entities']])\n",
    "    relations = \"\\n\".join([f'\"{relations}\"' for relations in data_point['relations']])\n",
    "\n",
    "    text = f\"Given the following text, identify and extract all entities and their relations. Query: {query}\\n Entities: [{entities}]\\nRelations:\\n{relations}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding all the layers that LORA can be applied to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Gemma 7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cuda()\n",
    "\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "cache_dir = \"SLM/gemma2b\"  # Specify an alternative cache directory\n",
    "dataset_name = \"Babelscape/SREDFM\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, cache_dir)\n",
    "dataset = load_and_clean_dataset(dataset_name, cache_dir)\n",
    "\n",
    "tokenized_datasets = dataset.map(lambda dp: {\"model_input\": preprocess_function(dp)})\n",
    "logger.info(tokenized_datasets[\"test\"][0])\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "logger.info(modules)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "logger.info(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    dataset_text_field='model_input',\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,  # Increase batch size if memory allows\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=100,  # Add warmup steps\n",
    "        max_steps=1000,  # Increase max steps for better training\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,  # Log less frequently to reduce overhead\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,  # Use mixed precision training\n",
    "        dataloader_num_workers=4,  # Optimize data loading\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "new_model = \"gemma2b-trained\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    cache_dir=\"SLM/gemma2b_trained\"  # Specify an alternative cache directory\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.to(\"cpu\")\n",
    "tokenizer.save_pretrained(\"gemma2b_trained\")\n",
    "merged_model.save_pretrained(\"gemma2b_trained\", safe_serialization=True)\n",
    "\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Gemma7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cuda()\n",
    "\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "cache_dir = \"SLM/gemma7b\"  # Specify an alternative cache directory\n",
    "dataset_name = \"Babelscape/SREDFM\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id, cache_dir)\n",
    "dataset = load_and_clean_dataset(dataset_name, cache_dir)\n",
    "\n",
    "tokenized_datasets = dataset.map(lambda dp: {\"model_input\": preprocess_function(dp)})\n",
    "logger.info(tokenized_datasets[\"test\"][0])\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "logger.info(modules)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "logger.info(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    dataset_text_field='model_input',\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,  # Increase batch size if memory allows\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=100,  # Add warmup steps\n",
    "        max_steps=1000,  # Increase max steps for better training\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,  # Log less frequently to reduce overhead\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,  # Use mixed precision training\n",
    "        dataloader_num_workers=4,  # Optimize data loading\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "new_model = \"gemma7b-trained\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    cache_dir=\"SLM/gemma7b_trained\"  # Specify an alternative cache directory\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.to(\"cpu\")\n",
    "tokenizer.save_pretrained(\"gemma7b_trained\")\n",
    "merged_model.save_pretrained(\"gemma7b_trained\", safe_serialization=True)\n",
    "\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling model results into a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models have been pushed to hugging-face and can be pulled from there for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a dataloader for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_batch(queries, model, tokenizer, device):\n",
    "    prompt_template = \"\"\"\n",
    "    <start_of_turn>user\n",
    "    Given the following text, identify and extract all entities and their relations.\n",
    "    {query}\n",
    "    <end_of_turn>\\n<start_of_turn>model\n",
    "    \"\"\"\n",
    "    # Generate prompts\n",
    "    prompts = [prompt_template.format(query=query) for query in queries]\n",
    "    encodeds = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",  # Explicitly specify padding\n",
    "        truncation=True,\n",
    "        max_length=1024  # Adjust based on your model's max input length\n",
    "    )\n",
    "    encodeds = {key: value.to(device) for key, value in encodeds.items()}\n",
    "\n",
    "    # Mixed precision inference for speedup\n",
    "    generated_ids = model.generate(\n",
    "            **encodeds,\n",
    "            max_new_tokens=500,  # Adjust as per expected output length\n",
    "            do_sample=False,     # Disable sampling for faster, deterministic results\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    outputs = []\n",
    "    for ids in generated_ids:\n",
    "        full_output = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        if \"Entities and Relations:\" in full_output:\n",
    "            relevant_part = full_output.split(\"Entities and Relations:\")[1].strip()\n",
    "            outputs.append(relevant_part)\n",
    "        else:\n",
    "            outputs.append(full_output.strip())  # Fallback to entire output if section is missing\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading first 1000 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Babelscape/SREDFM\", \"en\", cache_dir=\"SLM/datasets\", trust_remote_code=True)\n",
    "test_dataset = dataset['test'].select(range(1000))  # Select the first 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, true_entities, true_relations):\n",
    "        self.texts = texts\n",
    "        self.true_entities = true_entities\n",
    "        self.true_relations = true_relations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"true_entities\": self.true_entities[idx],\n",
    "            \"true_relations\": self.true_relations[idx],\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    true_entities = [item[\"true_entities\"] for item in batch]\n",
    "    true_relations = [item[\"true_relations\"] for item in batch]\n",
    "    return texts, true_entities, true_relations\n",
    "\n",
    "query_dataset = QueryDataset(\n",
    "    texts=test_dataset['text'],\n",
    "    true_entities=test_dataset['entities'],\n",
    "    true_relations=test_dataset['relations']\n",
    ")\n",
    "batch_size = 16  # Adjust based on available GPU memory\n",
    "dataloader = DataLoader(query_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 2b inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_path = \"Chinmay0701/gemma2b-trained\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=\"gemma2b-trained\", padding_size = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "tokenizer.padding_side = \"left\"  # Fix padding side for decoder-only models\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32, \n",
    "    cache_dir=\"gemma2b-trained\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "output_file = \"gemma2b_entities_relations.csv\"\n",
    "\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Query\", \"Predicted Entities\", \"Predicted Relations\", \"Output\"])  # Write header row\n",
    "\n",
    "    for batch_idx, (queries, true_entities, true_relations) in enumerate(dataloader):\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(dataloader)}...\")\n",
    "        \n",
    "        # Get predictions for the batch\n",
    "        predicted = get_completion_batch(queries, model, tokenizer, device)\n",
    "\n",
    "        # Split predicted results into entities and relations\n",
    "        for query, pred_output, true_ent, true_rel in zip(queries, predicted, true_entities, true_relations):\n",
    "            if \"Entities:\" in pred_output and \"Relations:\" in pred_output:\n",
    "            # Split the predicted output into entities and relations parts\n",
    "                parts = pred_output.split(\"Relations:\")\n",
    "                pred_entities = parts[0].split(\"Entities:\")\n",
    "                pred_entities = pred_entities[1].replace(\"Entities:\", \"\").strip()\n",
    "                pred_relations = parts[1].strip()\n",
    "            else:\n",
    "                pred_entities = \"NA\"\n",
    "                pred_relations = \"NA\"\n",
    "            \n",
    "            writer.writerow([query,pred_output ,pred_entities, pred_relations])\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 7b Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Chinmay0701/gemma7b-trained\"  # Path where the model and tokenizer are saved\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=\"gemma7b-trained\", padding_size = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "tokenizer.padding_side = \"left\" \n",
    "\n",
    "output_file = \"gemma7b_entities_relations.csv\"\n",
    "\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Query\", \"Predicted Entities\", \"Predicted Relations\", \"Output\"])  # Write header row\n",
    "\n",
    "    for batch_idx, (queries, true_entities, true_relations) in enumerate(dataloader):\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(dataloader)}...\")\n",
    "        \n",
    "        # Get predictions for the batch\n",
    "        predicted = get_completion_batch(queries, model, tokenizer, device)\n",
    "\n",
    "        # Split predicted results into entities and relations\n",
    "        for query, pred_output, true_ent, true_rel in zip(queries, predicted, true_entities, true_relations):\n",
    "            if \"Entities:\" in pred_output and \"Relations:\" in pred_output:\n",
    "            # Split the predicted output into entities and relations parts\n",
    "                parts = pred_output.split(\"Relations:\")\n",
    "                pred_entities = parts[0].split(\"Entities:\")\n",
    "                pred_entities = pred_entities[1].replace(\"Entities:\", \"\").strip()\n",
    "                pred_relations = parts[1].strip()\n",
    "            else:\n",
    "                pred_entities = \"NA\"\n",
    "                pred_relations = \"NA\"\n",
    "            \n",
    "            writer.writerow([query,pred_output ,pred_entities, pred_relations])\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_file}.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
